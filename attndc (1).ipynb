{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oBU8jyavFWkg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b431fb19-fa93-4bf7-9884-5a37a687001e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting face_recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (8.1.7)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (19.24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_recognition) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from face_recognition) (9.4.0)\n",
            "Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566164 sha256=13d8395cdf0d25274a1ce05400d13aec107868d884384c1834f916f2186ef741\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/eb/cf/e9eced74122b679557f597bb7c8e4c739cfcac526db1fd523d\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face_recognition\n",
            "Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization , Dropout\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "!pip install face_recognition\n",
        "import face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file = zipfile.ZipFile('datasetimages3.zip')"
      ],
      "metadata": {
        "id": "JY9Afc1tHcfv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file.extractall('datasetimages3')"
      ],
      "metadata": {
        "id": "myEtBTw4K9Qa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_dir = '/content/datasetimages2/datasetimages3/train'\n",
        "test_data_dir = '/content/datasetimages2/datasetimages3/test'"
      ],
      "metadata": {
        "id": "QxF47QkxK-0E"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess training images without ImageDataGenerator\n",
        "train_images = []\n",
        "train_labels = []\n",
        "\n",
        "for filename in os.listdir(train_data_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        img_path = os.path.join(train_data_dir, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Preprocess image: resize, normalization, etc.\n",
        "        img = cv2.resize(img, (224, 224))  # Example: resize to 224x224\n",
        "        img = img / 255.0  # Normalize pixel values to range [0, 1]\n",
        "\n",
        "        train_images.append(img)\n",
        "\n",
        "        # Extract label from filename (assuming a naming convention)\n",
        "        label = filename.split('_')[0]  # Example: Assuming labels are in filename\n",
        "        train_labels.append(label)\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "train_images = np.array(train_images)\n",
        "train_labels = np.array(train_labels)"
      ],
      "metadata": {
        "id": "sIVv39oULPhE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images = []\n",
        "test_labels = []\n",
        "\n",
        "for filename in os.listdir(train_data_dir):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        img_path = os.path.join(train_data_dir, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "         # Extract label from filename (assuming a naming convention)\n",
        "        label = filename.split('_')[0]  # Example: Assuming labels are in filename\n",
        "        test_labels.append(label)\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "test_images = np.array(test_images)\n",
        "test_labels = np.array(test_labels)"
      ],
      "metadata": {
        "id": "qyb-V0_mBy6f"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the image data generator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load the training and testing data\n",
        "training_set = train_datagen.flow_from_directory('/content/datasetimages3/datasetimages3/train',\n",
        "                                                 target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
        "test_set = test_datagen.flow_from_directory(test_data_dir, target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x=training_set, epochs=3, validation_data='/content/datasetimages3/datasetimages3/test')\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x=test_set, verbose=0)\n",
        "print('Loss: {}'.format(loss))\n",
        "print('Accuracy: {}'.format(accuracy))\n",
        "\n",
        "# Save the model\n",
        "model.save('image_classifier.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "collapsed": true,
        "id": "0923o4HD0sua",
        "outputId": "1d444287-1efa-423a-9c30-e8e5cf48332b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images belonging to 0 classes.\n",
            "Found 0 images belonging to 0 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Must provide at least one structure",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-80e82588458a>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/datasetimages3/datasetimages3/test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/tree/optree_impl.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"`func` must be callable. Received: func={func}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstructures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Must provide at least one structure\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mother\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstructures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Must provide at least one structure"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WBGNv8fu2orT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}